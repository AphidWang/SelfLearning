import { ChatOpenAI } from '@langchain/openai';
import { ChatPromptTemplate, MessagesPlaceholder } from '@langchain/core/prompts';
import { LLMChain } from 'langchain/chains';
import { BaseLLM } from '@langchain/core/language_models/llms';
import { ChatMessage, ChatResponse } from '../types';
import { chatConfig, DEFAULT_SYSTEM_PROMPT } from '../config/index';
import { ConversationMemory } from './memory';
import { LLMResult } from '@langchain/core/outputs';
import { BaseLLMCallOptions } from '@langchain/core/language_models/llms';
import { api } from '../../../services/api';
import actions from '../config/actions.json';

// å°‡ actions è½‰æ›ç‚ºæ˜“è®€çš„æ ¼å¼
const actionsDescription = Object.entries(actions.actions as Record<string, {
  description: string;
  params: Record<string, {
    type: string;
    description: string;
    required?: boolean;
  }>;
  returns: { type: string };
}>)
  .map(([name, action]) => {
    const params = Object.entries(action.params)
      .map(([paramName, param]) => `  "${paramName}": {
    "type": "${param.type}",
    "description": "${param.description}"${param.required ? ',\n    "required": true' : ''}
  }`)
      .join(',\n');
    return `"${name}": {
  "description": "${action.description}",
  "parameters": {
${params}
  },
  "returns": {
    "type": "${action.returns.type}"
  }
}`;
  })
  .join(',\n');

const SYSTEM_PROMPT_WITH_ACTIONS = `${DEFAULT_SYSTEM_PROMPT}

Available actions:
{
${actionsDescription}
}

Please respond with a JSON object in the following format:
{
  "tool": "action_name",
  "params": {
    "param_name": "param_value"
  }
}`;

class CustomLLM extends BaseLLM {
  constructor() {
    super({
      concurrency: 1,
      maxConcurrency: 1,
      maxRetries: 3,
    });
  }

  async _generate(
    prompts: string[],
    options: BaseLLMCallOptions,
    runManager?: any
  ): Promise<LLMResult> {
    const generations = await Promise.all(
      prompts.map(async (prompt) => {
        const token = localStorage.getItem('token');
        const payload = {
          messages: [
            { type: 'system', content: SYSTEM_PROMPT_WITH_ACTIONS },
            { type: 'user', content: prompt }
          ],
          model: chatConfig.modelName,
          temperature: 0.7,
        };
        console.log('ğŸ“¤ Request payload:', payload);
        
        const response = await api.post('/api/chat/completions', payload, {
          headers: {
            'Authorization': `Bearer ${token}`,
            'Content-Type': 'application/json'
          },
          withCredentials: true
        });

        console.log('ğŸ“¥ Response:', response.data);
        const data = response.data;
        if (!data.choices || !data.choices[0]?.message?.content) {
          throw new Error('Invalid API response format');
        }
        return {
          text: data.choices[0].message.content,
          generationInfo: {
            finishReason: data.choices[0].finish_reason,
          },
        };
      })
    );

    return {
      generations: [generations],
    };
  }

  _llmType(): string {
    return 'custom';
  }
}

export class ChatService {
  private memory: ConversationMemory;
  private chain: LLMChain;

  constructor() {
    // åˆå§‹åŒ–è¨˜æ†¶
    this.memory = new ConversationMemory();

    // å»ºç«‹æç¤ºæ¨¡æ¿
    const prompt = ChatPromptTemplate.fromMessages([
      ['system', SYSTEM_PROMPT_WITH_ACTIONS],
      new MessagesPlaceholder('history'),
      ['human', '{input}'],
    ]);

    // å»ºç«‹å°è©±éˆ
    this.chain = new LLMChain({
      llm: new CustomLLM(),
      prompt,
    });
  }

  // ç™¼é€è¨Šæ¯ä¸¦ç²å–å›æ‡‰
  public async sendMessage(message: string): Promise<ChatResponse> {
    try {
      // æ·»åŠ ç”¨æˆ¶è¨Šæ¯åˆ°è¨˜æ†¶
      this.memory.addMessage('user', message);

      // æº–å‚™å°è©±æ­·å²
      const history = this.memory.getRecentHistory().map(msg => ({
        type: msg.role,
        content: msg.content,
      }));

      // ç²å–æ‘˜è¦
      const summary = this.memory.getSummary();

      // åŸ·è¡Œå°è©±éˆ
      const response = await this.chain.call({
        input: message,
        history,
        summary: summary || '',
      });

      console.log('ğŸ¤– AI Response:', response.text);

      // æ·»åŠ åŠ©æ‰‹å›æ‡‰åˆ°è¨˜æ†¶
      this.memory.addMessage('assistant', response.text);

      return {
        message: response.text,
      };
    } catch (error) {
      console.error('âŒ Chat error:', error);
      return {
        message: '',
        error: error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤',
      };
    }
  }

  // æ¸…é™¤å°è©±æ­·å²
  public clearHistory(): void {
    this.memory.clear();
  }
} 