import { ChatOpenAI } from '@langchain/openai';
import { ChatPromptTemplate, MessagesPlaceholder } from '@langchain/core/prompts';
import { LLMChain } from 'langchain/chains';
import { BaseLLM } from '@langchain/core/language_models/llms';
import { ChatMessage, ChatResponse } from '../types';
import { chatConfig } from '../config/index';
import { ConversationMemory } from './memory';
import { LLMResult } from '@langchain/core/outputs';
import { BaseLLMCallOptions } from '@langchain/core/language_models/llms';
import { api } from '../../../services/api';
import actions from '../config/actions.json';

const DEFAULT_SYSTEM_PROMPT = `ä½ æ˜¯ä¸€ä½æ™ºæ…§åŠ©ç†`;

// å°‡ actions è½‰æ›ç‚ºæ˜“è®€çš„æ ¼å¼
//console.log('ğŸ” Actions:', actions);
const actionsDescription = Object.entries(actions.actions)
  .map(([name, action]) => {
    const params = Object.entries(action.params)
      .map(([paramName, param]) => `${paramName}${(param as any).required ? ' (required)' : ''}: ${param.description} (${param.type})`)
      .join('\n    ');
    const returns = `${action.returns.type}${(action.returns as any).properties ? ` with properties: ${JSON.stringify((action.returns as any).properties)}` : ''}`;
    return `${name}:
  Description: ${action.description}
  Parameters:
    ${params}
  Returns: ${returns}`;
  })
  .join('\n\n');

const SYSTEM_PROMPT_WITH_ACTIONS = `${DEFAULT_SYSTEM_PROMPT}
===
ä½ å¯ä»¥æ ¹æ“šä½¿ç”¨è€…è¼¸å…¥ã€Œå»ºè­°ä¸€å€‹æœ€é©åˆçš„å‹•ä½œã€ï¼Œä¸¦æä¾›å»ºè­°çš„åƒæ•¸ã€‚

è«‹æ³¨æ„ï¼š
1. ä½ ä¸èƒ½ç›´æ¥åŸ·è¡Œå‹•ä½œï¼Œåªèƒ½ã€Œå»ºè­°æ‡‰è©²åšä»€éº¼å‹•ä½œã€ã€‚
2. ç³»çµ±æœƒæ ¹æ“šä½ çš„å»ºè­°å…§å®¹ï¼Œè«‹ä½¿ç”¨è€…ç¢ºèªæ˜¯å¦åŸ·è¡Œï¼ŒçœŸæ­£çš„åŸ·è¡Œæœƒç”±ç³»çµ±å®Œæˆã€‚
3. è«‹åœ¨å›æ‡‰ä¸­åŠ å…¥é¼“å‹µèˆ‡å¼•å°èªå¥ï¼Œä½†ä¸èƒ½èªªã€Œæˆ‘å·²ç¶“ç‚ºä½ å»ºç«‹äº†...ã€ã€ã€Œæˆ‘å·²ç¶“å®Œæˆ...ã€ï¼Œé€™æ¨£æœƒèª¤å°ä½¿ç”¨è€…ã€‚

å¯ç”¨çš„å‹•ä½œåˆ—è¡¨ï¼š

Available actions:
${actionsDescription}

Please respond with a JSON object in the following format:
{
  "tool": "action_name",
  "params": {
    "param_name": "param_value"
  },
  "message": "Your response message to the user"
}`.replace(/{/g, '{{').replace(/}/g, '}}');

//console.log('ğŸ“ SYSTEM_PROMPT_WITH_ACTIONS: ', SYSTEM_PROMPT_WITH_ACTIONS);

class CustomLLM extends BaseLLM {
  constructor() {
    super({
      concurrency: 1,
      maxConcurrency: 1,
      maxRetries: 3,
    });
  }

  async _generate(
    prompts: string[],
    options: BaseLLMCallOptions,
    runManager?: any
  ): Promise<LLMResult> {
    const generations = await Promise.all(
      prompts.map(async (prompt) => {
        const token = localStorage.getItem('token');
        const payload = {
          messages: [
            { type: 'system', content: SYSTEM_PROMPT_WITH_ACTIONS },
            { type: 'user', content: prompt }
          ],
          model: chatConfig.modelName,
          temperature: 0.7,
        };
        console.log('ğŸ“¤ Request payload:', payload);
        
        const response = await api.post('/api/chat/completions', payload, {
          headers: {
            'Authorization': `Bearer ${token}`,
            'Content-Type': 'application/json'
          },
          withCredentials: true
        });

        console.log('ğŸ“¥ Response:', response.data);
        const data = response.data;
        if (!data.choices || !data.choices[0]?.message?.content) {
          throw new Error('Invalid API response format');
        }
        return {
          text: data.choices[0].message.content,
          generationInfo: {
            finishReason: data.choices[0].finish_reason,
          },
        };
      })
    );

    return {
      generations: [generations],
    };
  }

  _llmType(): string {
    return 'custom';
  }
}

export class ChatService {
  private memory: ConversationMemory;
  private chain: LLMChain;

  constructor() {
    // åˆå§‹åŒ–è¨˜æ†¶
    this.memory = new ConversationMemory();

    // å»ºç«‹æç¤ºæ¨¡æ¿
    const prompt = ChatPromptTemplate.fromMessages([
      ['system', SYSTEM_PROMPT_WITH_ACTIONS],
      new MessagesPlaceholder('history'),
      ['human', '{input}']
    ]);

    // å»ºç«‹å°è©±éˆ
    this.chain = new LLMChain({
      llm: new CustomLLM(),
      prompt,
      verbose: true
    });
  }

  // ç™¼é€è¨Šæ¯ä¸¦ç²å–å›æ‡‰
  public async sendMessage(message: string): Promise<ChatResponse> {
    try {
      // æ·»åŠ ç”¨æˆ¶è¨Šæ¯åˆ°è¨˜æ†¶
      this.memory.addMessage('user', message);

      // æº–å‚™å°è©±æ­·å²
      const history = this.memory.getRecentHistory().map(msg => ({
        type: msg.role,
        content: msg.content,
      }));

      // ç²å–æ‘˜è¦
      const summary = this.memory.getSummary();

      // åŸ·è¡Œå°è©±éˆ
      const response = await this.chain.call({
        input: message,
        history,
        summary: summary || '',
      });

      console.log('ğŸ¤– AI Response:', response.text);

      // æ·»åŠ åŠ©æ‰‹å›æ‡‰åˆ°è¨˜æ†¶
      this.memory.addMessage('assistant', response.text);

      return {
        message: response.text,
      };
    } catch (error) {
      console.error('âŒ Chat error:', error);
      return {
        message: '',
        error: error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤',
      };
    }
  }

  // æ¸…é™¤å°è©±æ­·å²
  public clearHistory(): void {
    this.memory.clear();
  }
} 